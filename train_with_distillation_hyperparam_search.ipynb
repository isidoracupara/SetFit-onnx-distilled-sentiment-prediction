{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer, DistillationSetFitTrainer, sample_dataset\n",
    "from setfit.exporters.onnx import export_onnx\n",
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/isidoracupara/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 337.99it/s]\n",
      "Loading cached shuffled indices for dataset at /Users/isidoracupara/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-2459d1a782cafb86.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/isidoracupara/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-7a06a0835566eb48.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "train_dataset_teacher = sample_dataset(dataset[\"train\"],label_column=\"label\")\n",
    "train_dataset_student = dataset[\"train\"].shuffle(seed=0).select(range(500))\n",
    "eval_dataset = dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 645/645 [00:00<00:00, 427kB/s]\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_id = [\"sentence-transformers/paraphrase-mpnet-base-v2\", \"paraphrase-multilingual-MiniLM-L12-v2\"]\n",
    "\n",
    "\n",
    "def make_model_teacher(params=None):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(\n",
    "        \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "def make_model_student(params=None):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    )\n",
    "\n",
    "teacher_model_init = make_model_teacher()\n",
    "student_model_init = make_model_student()\n",
    "\n",
    "# make_model = list(map(make_model(), model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search_function(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 5),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \"num_iterations\": trial.suggest_categorical(\"num_iterations\", [5, 10, 20]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "[I 2023-07-24 11:31:46,118] A new study created in memory with name: no-name-5b4ff039-c06a-46c2-9173-b63513e87890\n",
      "Trial: {'learning_rate': 3.1670840248189388e-06, 'num_epochs': 2, 'batch_size': 8, 'seed': 15, 'num_iterations': 20}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 4088.02it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num epochs = 2\n",
      "  Total optimization steps = 160\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 80/80 [01:12<00:00,  1.11it/s]\n",
      "Iteration: 100%|██████████| 80/80 [01:11<00:00,  1.12it/s]\n",
      "Epoch: 100%|██████████| 2/2 [02:23<00:00, 71.80s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:34:23,335] Trial 0 finished with value: 0.8623853211009175 and parameters: {'learning_rate': 3.1670840248189388e-06, 'num_epochs': 2, 'batch_size': 8, 'seed': 15, 'num_iterations': 20}. Best is trial 0 with value: 0.8623853211009175.\n",
      "Trial: {'learning_rate': 5.64868167030926e-06, 'num_epochs': 3, 'batch_size': 8, 'seed': 40, 'num_iterations': 10}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 2752.53it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num epochs = 3\n",
      "  Total optimization steps = 120\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 40/40 [00:36<00:00,  1.10it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:35<00:00,  1.14it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:36<00:00,  1.10it/s]\n",
      "Epoch: 100%|██████████| 3/3 [01:47<00:00, 35.97s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:36:24,393] Trial 1 finished with value: 0.8612385321100917 and parameters: {'learning_rate': 5.64868167030926e-06, 'num_epochs': 3, 'batch_size': 8, 'seed': 40, 'num_iterations': 10}. Best is trial 0 with value: 0.8623853211009175.\n",
      "Trial: {'learning_rate': 5.517222812850763e-06, 'num_epochs': 1, 'batch_size': 16, 'seed': 27, 'num_iterations': 5}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 5/5 [00:00<00:00, 3272.20it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 160\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 10\n",
      "  Total train batch size = 16\n",
      "Iteration: 100%|██████████| 10/10 [00:15<00:00,  1.53s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:15<00:00, 15.27s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:36:53,340] Trial 2 finished with value: 0.8004587155963303 and parameters: {'learning_rate': 5.517222812850763e-06, 'num_epochs': 1, 'batch_size': 16, 'seed': 27, 'num_iterations': 5}. Best is trial 0 with value: 0.8623853211009175.\n",
      "Trial: {'learning_rate': 6.202400140351295e-06, 'num_epochs': 2, 'batch_size': 32, 'seed': 2, 'num_iterations': 5}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 5/5 [00:00<00:00, 3267.61it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 160\n",
      "  Num epochs = 2\n",
      "  Total optimization steps = 10\n",
      "  Total train batch size = 32\n",
      "Iteration: 100%|██████████| 5/5 [00:15<00:00,  3.15s/it]\n",
      "Iteration: 100%|██████████| 5/5 [00:14<00:00,  2.84s/it]\n",
      "Epoch: 100%|██████████| 2/2 [00:29<00:00, 14.97s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:37:37,474] Trial 3 finished with value: 0.8107798165137615 and parameters: {'learning_rate': 6.202400140351295e-06, 'num_epochs': 2, 'batch_size': 32, 'seed': 2, 'num_iterations': 5}. Best is trial 0 with value: 0.8623853211009175.\n",
      "Trial: {'learning_rate': 1.3727181128222151e-05, 'num_epochs': 1, 'batch_size': 8, 'seed': 3, 'num_iterations': 20}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 3472.97it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 80\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 80/80 [01:13<00:00,  1.09it/s]\n",
      "Epoch: 100%|██████████| 1/1 [01:13<00:00, 73.16s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:39:06,932] Trial 4 finished with value: 0.8681192660550459 and parameters: {'learning_rate': 1.3727181128222151e-05, 'num_epochs': 1, 'batch_size': 8, 'seed': 3, 'num_iterations': 20}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 6.408373330076347e-05, 'num_epochs': 1, 'batch_size': 8, 'seed': 30, 'num_iterations': 5}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 5/5 [00:00<00:00, 3308.85it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 160\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 20\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 20/20 [00:19<00:00,  1.05it/s]\n",
      "Epoch: 100%|██████████| 1/1 [00:19<00:00, 19.06s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:39:41,515] Trial 5 finished with value: 0.7775229357798165 and parameters: {'learning_rate': 6.408373330076347e-05, 'num_epochs': 1, 'batch_size': 8, 'seed': 30, 'num_iterations': 5}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 8.702315031732868e-05, 'num_epochs': 4, 'batch_size': 4, 'seed': 31, 'num_iterations': 20}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 3234.10it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num epochs = 4\n",
      "  Total optimization steps = 640\n",
      "  Total train batch size = 4\n",
      "Iteration: 100%|██████████| 160/160 [01:34<00:00,  1.69it/s]\n",
      "Iteration: 100%|██████████| 160/160 [01:35<00:00,  1.68it/s]\n",
      "Iteration: 100%|██████████| 160/160 [01:32<00:00,  1.73it/s]\n",
      "Iteration: 100%|██████████| 160/160 [01:33<00:00,  1.72it/s]\n",
      "Epoch: 100%|██████████| 4/4 [06:15<00:00, 93.88s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:46:11,821] Trial 6 finished with value: 0.7522935779816514 and parameters: {'learning_rate': 8.702315031732868e-05, 'num_epochs': 4, 'batch_size': 4, 'seed': 31, 'num_iterations': 20}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 4.998165420256818e-06, 'num_epochs': 4, 'batch_size': 8, 'seed': 17, 'num_iterations': 5}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 5/5 [00:00<00:00, 2817.24it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 160\n",
      "  Num epochs = 4\n",
      "  Total optimization steps = 80\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n",
      "Iteration: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
      "Iteration: 100%|██████████| 20/20 [00:17<00:00,  1.11it/s]\n",
      "Iteration: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
      "Epoch: 100%|██████████| 4/4 [01:11<00:00, 17.83s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:47:38,619] Trial 7 finished with value: 0.8474770642201835 and parameters: {'learning_rate': 4.998165420256818e-06, 'num_epochs': 4, 'batch_size': 8, 'seed': 17, 'num_iterations': 5}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 8.62849643915148e-06, 'num_epochs': 4, 'batch_size': 8, 'seed': 5, 'num_iterations': 10}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 2818.56it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num epochs = 4\n",
      "  Total optimization steps = 160\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 40/40 [00:35<00:00,  1.12it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:37<00:00,  1.08it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:36<00:00,  1.11it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:36<00:00,  1.08it/s]\n",
      "Epoch: 100%|██████████| 4/4 [02:25<00:00, 36.42s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:50:19,566] Trial 8 finished with value: 0.8669724770642202 and parameters: {'learning_rate': 8.62849643915148e-06, 'num_epochs': 4, 'batch_size': 8, 'seed': 5, 'num_iterations': 10}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 2.983678560404416e-05, 'num_epochs': 1, 'batch_size': 4, 'seed': 27, 'num_iterations': 5}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 5/5 [00:00<00:00, 2945.02it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 160\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 40\n",
      "  Total train batch size = 4\n",
      "Iteration: 100%|██████████| 40/40 [00:24<00:00,  1.65it/s]\n",
      "Epoch: 100%|██████████| 1/1 [00:24<00:00, 24.21s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:50:59,033] Trial 9 finished with value: 0.8635321100917431 and parameters: {'learning_rate': 2.983678560404416e-05, 'num_epochs': 1, 'batch_size': 4, 'seed': 27, 'num_iterations': 5}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 1.6910170338204101e-06, 'num_epochs': 5, 'batch_size': 64, 'seed': 9, 'num_iterations': 20}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 3494.82it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num epochs = 5\n",
      "  Total optimization steps = 50\n",
      "  Total train batch size = 64\n",
      "Iteration: 100%|██████████| 10/10 [01:07<00:00,  6.75s/it]\n",
      "Iteration: 100%|██████████| 10/10 [01:08<00:00,  6.81s/it]\n",
      "Iteration: 100%|██████████| 10/10 [01:07<00:00,  6.73s/it]\n",
      "Iteration: 100%|██████████| 10/10 [01:04<00:00,  6.47s/it]\n",
      "Iteration: 100%|██████████| 10/10 [01:09<00:00,  6.93s/it]\n",
      "Epoch: 100%|██████████| 5/5 [05:36<00:00, 67.36s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:56:50,579] Trial 10 finished with value: 0.8176605504587156 and parameters: {'learning_rate': 1.6910170338204101e-06, 'num_epochs': 5, 'batch_size': 64, 'seed': 9, 'num_iterations': 20}. Best is trial 4 with value: 0.8681192660550459.\n",
      "Trial: {'learning_rate': 1.550327833138422e-05, 'num_epochs': 4, 'batch_size': 8, 'seed': 5, 'num_iterations': 10}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 3031.22it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num epochs = 4\n",
      "  Total optimization steps = 160\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 40/40 [00:37<00:00,  1.07it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:37<00:00,  1.05it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:39<00:00,  1.02it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:37<00:00,  1.07it/s]\n",
      "Epoch: 100%|██████████| 4/4 [02:32<00:00, 38.05s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 11:59:39,003] Trial 11 finished with value: 0.8704128440366973 and parameters: {'learning_rate': 1.550327833138422e-05, 'num_epochs': 4, 'batch_size': 8, 'seed': 5, 'num_iterations': 10}. Best is trial 11 with value: 0.8704128440366973.\n",
      "Trial: {'learning_rate': 1.7601525685153275e-05, 'num_epochs': 3, 'batch_size': 32, 'seed': 10, 'num_iterations': 10}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 3043.10it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num epochs = 3\n",
      "  Total optimization steps = 30\n",
      "  Total train batch size = 32\n",
      "Iteration: 100%|██████████| 10/10 [00:33<00:00,  3.34s/it]\n",
      "Iteration: 100%|██████████| 10/10 [00:33<00:00,  3.31s/it]\n",
      "Iteration: 100%|██████████| 10/10 [00:34<00:00,  3.40s/it]\n",
      "Epoch: 100%|██████████| 3/3 [01:40<00:00, 33.53s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 12:01:35,683] Trial 12 finished with value: 0.8646788990825688 and parameters: {'learning_rate': 1.7601525685153275e-05, 'num_epochs': 3, 'batch_size': 32, 'seed': 10, 'num_iterations': 10}. Best is trial 11 with value: 0.8704128440366973.\n",
      "Trial: {'learning_rate': 1.7389580590156273e-05, 'num_epochs': 5, 'batch_size': 64, 'seed': 8, 'num_iterations': 10}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 3253.16it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num epochs = 5\n",
      "  Total optimization steps = 25\n",
      "  Total train batch size = 64\n",
      "Iteration: 100%|██████████| 5/5 [00:36<00:00,  7.35s/it]\n",
      "Iteration: 100%|██████████| 5/5 [00:34<00:00,  6.90s/it]\n",
      "Iteration: 100%|██████████| 5/5 [00:34<00:00,  6.98s/it]\n",
      "Iteration: 100%|██████████| 5/5 [00:37<00:00,  7.49s/it]\n",
      "Iteration: 100%|██████████| 5/5 [00:34<00:00,  6.95s/it]\n",
      "Epoch: 100%|██████████| 5/5 [02:58<00:00, 35.67s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 12:04:49,724] Trial 13 finished with value: 0.8623853211009175 and parameters: {'learning_rate': 1.7389580590156273e-05, 'num_epochs': 5, 'batch_size': 64, 'seed': 8, 'num_iterations': 10}. Best is trial 11 with value: 0.8704128440366973.\n",
      "Trial: {'learning_rate': 1.550907479705045e-05, 'num_epochs': 2, 'batch_size': 16, 'seed': 1, 'num_iterations': 20}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 3469.23it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num epochs = 2\n",
      "  Total optimization steps = 80\n",
      "  Total train batch size = 16\n",
      "Iteration: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Iteration: 100%|██████████| 40/40 [01:08<00:00,  1.71s/it]\n",
      "Epoch: 100%|██████████| 2/2 [02:13<00:00, 66.74s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 12:07:19,276] Trial 14 finished with value: 0.8555045871559633 and parameters: {'learning_rate': 1.550907479705045e-05, 'num_epochs': 2, 'batch_size': 16, 'seed': 1, 'num_iterations': 20}. Best is trial 11 with value: 0.8704128440366973.\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 2626.04it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num epochs = 4\n",
      "  Total optimization steps = 160\n",
      "  Total train batch size = 8\n",
      "Iteration: 100%|██████████| 40/40 [00:39<00:00,  1.03it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:38<00:00,  1.05it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:39<00:00,  1.02it/s]\n",
      "Iteration: 100%|██████████| 40/40 [00:38<00:00,  1.03it/s]\n",
      "Epoch: 100%|██████████| 4/4 [02:35<00:00, 38.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐈‍⬛ Teacher traininer metrics: 0.8704128440366973\n"
     ]
    }
   ],
   "source": [
    "def train_teacher_model():\n",
    "\n",
    "    # Create trainer for teacher model\n",
    "    teacher_trainer = SetFitTrainer(\n",
    "        model_init= teacher_model_init,\n",
    "        train_dataset=train_dataset_teacher,\n",
    "        eval_dataset=eval_dataset,\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        metric=\"accuracy\",\n",
    "        batch_size=16,\n",
    "        num_iterations=20, # The number of text pairs to generate for contrastive learning\n",
    "        num_epochs=1, #A good rule of thumb is to start with a value that is 3 times the number of features in your data\n",
    "        # Excerpt from the research paper: \"...perform a hyperparameter search on the number of epochs in the range [25,75] and pick the best performing model on a validation split\"\n",
    "        column_mapping={\"sentence\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    "        )\n",
    "\n",
    "    # Train and evaluate\n",
    "\n",
    "    best_teacher = teacher_trainer.hyperparameter_search(hyperparameter_search_function, n_trials=15)\n",
    "    teacher_trainer.apply_hyperparameters(best_teacher.hyperparameters, final_model=True)\n",
    "    teacher_trainer.train()\n",
    "    print(f\"🐈‍⬛ Teacher traininer metrics: {best_teacher.objective}\")\n",
    "    # plot_param_importances(best_teacher.backend)\n",
    "\n",
    "    return teacher_trainer\n",
    "\n",
    "teacher_trainer = train_teacher_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "To use hyperparameter search, you need to pass your model through a model_init function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# plot_param_importances(best_student.backend)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m student_trainer\n\u001b[0;32m---> 32\u001b[0m student_trainer \u001b[39m=\u001b[39m train_student_model(teacher_model_init)\n",
      "Cell \u001b[0;32mIn[38], line 23\u001b[0m, in \u001b[0;36mtrain_student_model\u001b[0;34m(teacher_model_init)\u001b[0m\n\u001b[1;32m      7\u001b[0m student_trainer \u001b[39m=\u001b[39m DistillationSetFitTrainer(\n\u001b[1;32m      8\u001b[0m     teacher_model\u001b[39m=\u001b[39mteacher_model,\n\u001b[1;32m      9\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset_student,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     column_mapping\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m} \u001b[39m# Map dataset columns to text/label expected by trainer\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39m# Train and evaluate\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m best_student \u001b[39m=\u001b[39m student_trainer\u001b[39m.\u001b[39;49mhyperparameter_search(hyperparameter_search_function, n_trials\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m student_trainer\u001b[39m.\u001b[39mapply_hyperparameters(best_student\u001b[39m.\u001b[39mhyperparameters, final_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m student_trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Publican BE /Publican-Be-SetFit-scripts/venv/lib/python3.10/site-packages/setfit/trainer.py:512\u001b[0m, in \u001b[0;36mSetFitTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_search_backend \u001b[39m=\u001b[39m backend\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use hyperparameter search, you need to pass your model through a model_init function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_space \u001b[39m=\u001b[39m default_hp_space_optuna \u001b[39mif\u001b[39;00m hp_space \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m hp_space\n\u001b[1;32m    517\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_name \u001b[39m=\u001b[39m hp_name\n",
      "\u001b[0;31mRuntimeError\u001b[0m: To use hyperparameter search, you need to pass your model through a model_init function."
     ]
    }
   ],
   "source": [
    "# def train_student_model(teacher_model_init):\n",
    "\n",
    "#     teacher_model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "#     student_model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "#     # Create trainer for knowledge distillation\n",
    "#     student_trainer = DistillationSetFitTrainer(\n",
    "#         teacher_model=teacher_model,\n",
    "#         train_dataset=train_dataset_student,\n",
    "#         student_model=student_model,\n",
    "#         eval_dataset=eval_dataset,\n",
    "#         # model_init=student_model_init,\n",
    "#         loss_class=CosineSimilarityLoss,\n",
    "#         metric=\"accuracy\",\n",
    "#         batch_size=16,\n",
    "#         num_iterations=20,\n",
    "#         num_epochs=1,\n",
    "#         column_mapping={\"sentence\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    "#     )\n",
    "\n",
    "#     # Train and evaluate\n",
    "\n",
    "#     best_student = student_trainer.hyperparameter_search(hyperparameter_search_function, n_trials=15)\n",
    "#     student_trainer.apply_hyperparameters(best_student.hyperparameters, final_model=True)\n",
    "#     student_trainer.train()\n",
    "#     # print(f\"🐈‍⬛ Student traininer hyperparameters best: {best_student}\")\n",
    "#     print(f\"🐈‍⬛ Student traininer metrics: {best_student.objective}\")\n",
    "#     # plot_param_importances(best_student.backend)\n",
    "\n",
    "#     return student_trainer\n",
    "\n",
    "# student_trainer = train_student_model(teacher_model_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "[I 2023-07-24 15:23:45,090] A new study created in memory with name: no-name-9b75bdaa-d0da-4a83-9822-dd41eb2ae010\n",
      "Trial: {'learning_rate': 2.0525220061066955e-05, 'num_epochs': 5, 'batch_size': 32, 'seed': 32, 'num_iterations': 10}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 10/10 [00:00<00:00, 191.88it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num epochs = 5\n",
      "  Total optimization steps = 1565\n",
      "  Total train batch size = 32\n",
      "Iteration: 100%|██████████| 313/313 [09:36<00:00,  1.84s/it]\n",
      "Iteration: 100%|██████████| 313/313 [09:43<00:00,  1.86s/it]\n",
      "Iteration: 100%|██████████| 313/313 [09:58<00:00,  1.91s/it]\n",
      "Iteration: 100%|██████████| 313/313 [10:04<00:00,  1.93s/it]\n",
      "Iteration: 100%|██████████| 313/313 [10:11<00:00,  1.95s/it]\n",
      "Epoch: 100%|██████████| 5/5 [49:35<00:00, 595.04s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 16:13:32,453] Trial 0 finished with value: 0.8405963302752294 and parameters: {'learning_rate': 2.0525220061066955e-05, 'num_epochs': 5, 'batch_size': 32, 'seed': 32, 'num_iterations': 10}. Best is trial 0 with value: 0.8405963302752294.\n",
      "Trial: {'learning_rate': 8.569957170485215e-06, 'num_epochs': 3, 'batch_size': 4, 'seed': 36, 'num_iterations': 5}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 5/5 [00:00<00:00, 185.49it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num epochs = 3\n",
      "  Total optimization steps = 3750\n",
      "  Total train batch size = 4\n",
      "Iteration: 100%|██████████| 1250/1250 [10:26<00:00,  1.99it/s]\n",
      "Iteration: 100%|██████████| 1250/1250 [09:06<00:00,  2.29it/s]\n",
      "Iteration: 100%|██████████| 1250/1250 [09:01<00:00,  2.31it/s]\n",
      "Epoch: 100%|██████████| 3/3 [28:34<00:00, 571.55s/it]\n",
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "[I 2023-07-24 16:42:17,867] Trial 1 finished with value: 0.841743119266055 and parameters: {'learning_rate': 8.569957170485215e-06, 'num_epochs': 3, 'batch_size': 4, 'seed': 36, 'num_iterations': 5}. Best is trial 1 with value: 0.841743119266055.\n",
      "Trial: {'learning_rate': 5.769417862992356e-05, 'num_epochs': 5, 'batch_size': 64, 'seed': 6, 'num_iterations': 20}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "Generating Training Pairs: 100%|██████████| 20/20 [00:00<00:00, 206.36it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 20000\n",
      "  Num epochs = 5\n",
      "  Total optimization steps = 1565\n",
      "  Total train batch size = 64\n",
      "Iteration: 100%|██████████| 313/313 [19:32<00:00,  3.75s/it]\n",
      "Iteration: 100%|██████████| 313/313 [21:39<00:00,  4.15s/it]\n",
      "Iteration: 100%|██████████| 313/313 [22:36<00:00,  4.34s/it]\n",
      "Epoch:  60%|██████    | 3/5 [1:03:48<43:14, 1297.27s/it]"
     ]
    }
   ],
   "source": [
    "def train_student_model():\n",
    "\n",
    "    # Create trainer for knowledge distillation\n",
    "    student_trainer = SetFitTrainer(\n",
    "        train_dataset=train_dataset_student,\n",
    "        eval_dataset=eval_dataset,\n",
    "        model_init=make_model_student,\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        metric=\"accuracy\",\n",
    "        batch_size=16,\n",
    "        num_iterations=20,\n",
    "        num_epochs=1,\n",
    "        column_mapping={\"sentence\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "\n",
    "    best_student = student_trainer.hyperparameter_search(hyperparameter_search_function, n_trials=15)\n",
    "    student_trainer.apply_hyperparameters(best_student.hyperparameters, final_model=True)\n",
    "    student_trainer.train()\n",
    "    print(f\"🐈‍⬛ Student traininer hyperparameters best: {best_student}\")\n",
    "    print(f\"🐈‍⬛ Student traininer metrics: {best_student.objective}\")\n",
    "    # plot_param_importances(best_student.backend)\n",
    "\n",
    "    return student_trainer\n",
    "\n",
    "student_trainer = train_student_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_model(teacher_model, student_model):\n",
    "\n",
    "    student_model = SetFitModel.from_pretrained(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "    # Create trainer for knowledge distillation\n",
    "    student_trainer = DistillationSetFitTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        train_dataset=train_dataset_student,\n",
    "        student_model=student_model,\n",
    "        eval_dataset=eval_dataset,\n",
    "        # model_init=make_model_student,\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        metric=\"accuracy\",\n",
    "        batch_size=16,\n",
    "        num_iterations=20,\n",
    "        num_epochs=1,\n",
    "        column_mapping={\"sentence\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "\n",
    "    metrics = student_trainer.evaluate\n",
    "    student_trainer.train()\n",
    "    print(f\"🐈‍⬛ Student traininer metrics: {metrics}\")\n",
    "    # plot_param_importances(best_student.backend)\n",
    "\n",
    "    return student_trainer\n",
    "\n",
    "model = train_student_model(teacher_trainer,student_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model):\n",
    "\n",
    "    output_path = f\"model/setfit_model_distilled.onnx\"\n",
    "    export_onnx(model.model_body,\n",
    "                model.model_head,\n",
    "                opset=12,\n",
    "                output_path=output_path)\n",
    "    message = f\"Distilled model exported to onnx format.\\n\"\n",
    "    print(\"~\" * len(message) + \"\\n\" + message + \"~\" * len(message))\n",
    "\n",
    "export_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
